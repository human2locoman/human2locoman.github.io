<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning LocoMan skills from human demonstrations.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Human2LocoMan</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/human2locoman_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .human-data {
      color: #3A7E22; /* Choose your preferred color */
    }

    .robot-data {
      color: #8B51D2; /* Choose your preferred color */
    }
  </style>

</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yaruniu.com/">Yaru Niu</a><sup>1,*</sup>,</span>
            <span class="author-block">
              Yunzhe Zhang<sup>1,*</sup>,</span>
            <span class="author-block">
              Mingyang Yu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://linchangyi1.github.io/">Changyi Lin</a><sup>1</sup>,</span>
            <span class="author-block">
              Chenhao Li</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=7ZZ9fOIAAAAJ&hl=zh-CN">Yikai Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yxyang.github.io/">Yuxiang Yang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://wenhaoyu.weebly.com/">Wenhao Yu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://research.google/people/tingnanzhang/?&type=google">Tingnan Zhang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=LYt_2MgAAAAJ&hl=en">Bingqing Chen</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://jonfranc.com/">Jonathan Francis</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=6LYI6uUAAAAJ&hl=en">Zhenzhen Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.jie-tan.net/">Jie Tan</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.meche.engineering.cmu.edu/directory/bios/zhao-ding.html">Ding Zhao</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Carnegie Mellon University,</span>
            <span class="author-block"><sup>2</sup>Google DeepMind</span>
            <span class="author-block"><sup>3</sup>Bosch Center for Artificial Intelligence</span>
          </div>
          <div class="is-size-5 publication-authors" style="margin-top: 0.75em;">
            <span class="author-block">Robotics: Science and Systems (RSS) 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://human2bots.github.io/static/pdfs/human2locoman_rss_2025.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark disabled" role="button">
                  <!-- <span class="icon">
                      <i class="fab fa-github"></i>
                  </span> -->
                  <span>Code coming soon</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Quadrupedal robots have demonstrated impressive locomotion capabilities in complex environments, but equipping them with autonomous versatile manipulation skills in a scalable way remains a significant challenge. In this work, we introduce a cross-embodiment imitation learning system for quadrupedal manipulation, leveraging data collected from both humans and LocoMan, a quadruped equipped with multiple manipulation modes. Specifically, we develop a teleoperation and data collection pipeline, which unifies and modularizes the observation and action spaces of the human and the robot. To effectively leverage the collected data, we propose an efficient modularized architecture that supports co-training and pretraining on structured modality-aligned data across different embodiments. Additionally, we construct the first manipulation dataset for the LocoMan robot, covering various household tasks in both unimanual and bimanual modes, supplemented by a corresponding human dataset. We validate our system on six real-world manipulation tasks, where it achieves an average success rate improvement of 41.9% overall and 79.7% under out-of-distribution (OOD) settings compared to the baseline. Pretraining with human data contributes a 38.6% success rate improvement overall and 82.7% under OOD settings, enabling consistently better performance with only half the amount of robot data.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">System Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/images/system_overview.png"
               class="arch-image"
               alt="Learning Framework"/>
<p>
  Our system uses an XR headset for data collection, capturing egocentric 
  <span class="human-data">human data</span> and teleoperated 
  <span class="robot-data">robot data</span>, all mapped to a unified coordinate frame. 
  The dataset consists of aligned vision, proprioception, and actions from the 
  <span class="human-data">human</span> and the 
  <span class="robot-data">robot</span>. 
  We adopt a two-stage training process: the modularized cross-embodiment model is first pretrained on easy-to-collect 
  <span class="human-data">human data</span>, and then finetuned on a small amount of 
  <span class="robot-data">robot data</span>. 
  The resulting Human2LocoMan policies can be deployed on real robots for versatile manipulation tasks in both unimanual and bimanual modes.
</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column content">
        <h2 class="title is-3">Teleoperation & Data Collection System</h2>
        <div class="content has-text-justified">
          <p>
            <!-- Our teleoperation system is built upon <a href="https://robot-tv.github.io/">Open-Television</a>. By leveraging
            the head and hand tracking capabilities of the Apple Vision Pro, we enable immersive human teleoperation with 
            visual feedback on robot embodiments such as LocoMan. This system allows collecting data on the robot at least
            4x faster than using a traditional remote control. -->
            We develop a unified system for both robot teleoperation and human data collection.
            Our teleoperation system controls the 6D poses of end-effectors, gripper actions, and the rigid body where the stereo camera is mounted (the torso of LocoMan, the head of human). 
            The coordination of whole-body motions expand the workspace and camera sensing ranges. Our teleoperation system is fast-response, stable, and user frendly, 
            by incorporating interpolations for fast movements, handles and recovery from self-collisions, singularity, ground-collisions, and joint limits.
          </p>

          <!-- Wrap videos in a Bulma columns layout -->
          <div class="columns is-centered">
            <div class="column">
              <video id="main_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/toy_collect_bimanual_teleop.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="main_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/toy_collect_unimanual_teleop.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="robot_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/pour_bimanual_teleop.mp4" type="video/mp4">
              </video>
            </div>
          </div>

          <div class="columns is-centered">
            <div class="column">
              <video id="main_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/shoe_org_unimanual_teleop.mp4" type="video/mp4">
              </video>
            </div>
              <div class="column">
              <video id="main_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/new/tossing_teleop.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="main_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/drawer_org_unimanual_teleop.mp4" type="video/mp4">
              </video>
            </div>
          </div>

          <p>
            <!-- Apple Vision Pro is also used for collecting human videos with captured hand motions. Through the simple interface we designed,
            a human operator can easily collect dozens of trajectories by themselves within minutes. -->
            We mount a stereo camera on the VR set to collect first-person view of human videos and capture human's hand and head motions 
            within a unified coordinate frames as the robot embodiments.
            Through the simple interface we designed, a human operator without robot access can easily collect dozens of trajectories by themselves within minutes.
          </p>

          <div class="columns is-centered">
            <div class="column">
              <video id="main_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/toy_collect_bimanual_human.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="main_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/shoe_org_unimanual_human.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="robot_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/pour_bimanual_human.mp4" type="video/mp4">
              </video>
            </div>
          </div>

          <div class="columns is-centered">
            <div class="column">
              <video id="main_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/toy_collect_bimanual_human_fpv.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="main_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/shoe_org_unimanual_human_fpv.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="robot_demo" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/pour_bimanual_human_fpv.mp4" type="video/mp4">
              </video>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Modularized Cross-Embodiment Transformer (MXT)</h2>
        <div class="content has-text-justified">
          <p>
            Our learning framework is designed to efficiently utilize data from both human and robot sources, and account for modality-specific distributions unique to each embodiment. We propose a modularized design Modularized Cross-Embodiment Transformer (MXT). MXT consists mainly of three groups of modules: tokenizers, Transformer trunk, and detokenizers. The tokenizers act as encoders and map embodiment-specific observation modalities to tokens in the latent space, and the detokenizers translate the output tokens from the trunk to action modalities in the action space of each embodiment. The tokenizers and detokenizers are specific to one embodiment and are reinitialized for each new embodiment, while the trunk is shared across all embodiments and reused for transferring the policy among embodiments. 
          </p>
          <img src="./static/images/model_arch.png"
               class="arch-image"
               alt="Learning Framework"/>
        </div>
      </div>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h3 class="title is-4">MXT Policy Rollouts</h3>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/toy_collect_unimanual_rollout.mp4" type="video/mp4">
        </video>
        <!-- <h4 class="title is-5">Unimanual Toy Collection</h4> -->
      </div>
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/toy_collect_bimanual_rollout.mp4" type="video/mp4">
        </video>
        <!-- <h4 class="title is-5">Bimanual Toy Collection</h4> -->
      </div>
    </div>
      
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/shoe_org_unimanual_rollout.mp4" type="video/mp4">
        </video>
        <!-- <h4 class="title is-5">Unimanual Shoe Rack Organization</h4> -->
      </div>
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/new/mxt_bimanual_shoe_org.mp4" type="video/mp4">
        </video>
        <!-- <h4 class="title is-5">Bimanual Pouring</h4> -->
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/new/mxt_scoop.mp4" type="video/mp4">
        </video>
        <!-- <h4 class="title is-5">Unimanual Shoe Rack Organization</h4> -->
      </div>
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/new/mxt_60_pour_part1.mp4" type="video/mp4">
        </video>
        <!-- <h4 class="title is-5">Bimanual Pouring</h4> -->
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h3 class="title is-4">Robust to OOD Objects</h3>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/shoe_org_unimanual_ood_mxt.mp4" type="video/mp4">
        </video>
        <h4 class="title is-5">MXT</h4>
      </div>
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/shoe_org_unimanual_ood_mxt_no_pretrain.mp4" type="video/mp4">
        </video>
        <h4 class="title is-5">MXT (not pretrained)</h4>
      </div>
    </div>
      
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/new/mxt_60_pour_part2_ood.mp4" type="video/mp4">
        </video>
        <h4 class="title is-5">MXT</h4>
      </div>
      <div class="column">
        <video id="main_demo" autoplay controls muted loop playsinline width="100%", style="border-radius: 12px; overflow: hidden;">
          <source src="./static/videos/new/hit_60_pour_ood.mp4" type="video/mp4">
        </video>
        <h4 class="title is-5">HIT</h4>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h3 class="title is-4">Data-Efficient Finetuning</h3>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
        <div class="column">
          <video id="main_demo" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/mxt_2min_toy_collect_unimanual_rollout.mp4" type="video/mp4">
          </video>
          <h4 class="title is-5">MXT, finetuned with 2-min robot data (success 5/5)</h4>
        </div>
        <div class="column">
          <video id="main_demo" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/mxt_scratch_4min_toy_collect_unimanual_rollout.mp4" type="video/mp4">
          </video>
          <h4 class="title is-5">MXT (not pretrained), trained with 4-min robot data (success 2/5)</h4>
        </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h3 class="title is-4">Locomotion + Manipulation</h3>
      </div>
    </div>
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <video id="main_demo" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/new/loco_toy_collect_teaser.mp4" type="video/mp4">
          </video>
          <h4 class="title is-5">Unimanual toy collection with locomotion</h4>
        </div>
        <div class="column">
          <video id="main_demo" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/new/loco_shoe_org_teaser.mp4" type="video/mp4">
          </video>
          <h4 class="title is-5">Unimanual shoe organization with locomotion</h4>
        </div>
        <div class="column">
          <video id="robot_demo" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/new/loco_scoop1_teaser.mp4" type="video/mp4">
          </video>
          <h4 class="title is-5">Unimanual scooping with locomotion</h4>
        </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Quantitative Results</h2>
        <h3 class="title is-4">Evaluation on Six Real-World Tasks</h3>
        <div class="content has-text-justified">
<p>
  We report success rate (SR) ↑ in % and task score (TS) ↑ for each task.
  We highlight the best performance in <strong>bold</strong> and the second best in <u>underline</u>.
  ID results are based on 24 trials, and OOD results on 12 trials.
</p> 
          <img src="./static/images/main_results.png"
               class="arch-image"
               alt="Learning Framework"/>
<ul>
  <li>
    <strong>Robust versatile manipulation:</strong> MXT achieves an average success rate improvement of 41.9% overall and 79.7% under out-of-distribution (OOD) settings over six tasks compared to the baseline.
  </li>
  <li>
    <strong>Effective human data pretraining:</strong> Pretraining with human data contributes a 38.6% success rate improvement overall and 82.7% under OOD settings, enabling consistently better performance with only half the amount of robot data.
  </li>
</ul>
      </div>
        <h3 class="title is-4">Ablation Study</h3>
        <div class="content has-text-justified">
<p>
  We compare MXT, its ablation MXT-Agg, and the baseline HPT on SR and TS. 
  Here, <q>L</q> denotes the larger training set (40 trajectories for TC-Uni, 60 trajectories for TC-Bi), 
  while <q>S</q> denotes the smaller training set (20 trajectories for TC-Uni, 30 trajectories for TC-Bi).
</p>

          <img src="./static/images/ablation.png"
               class="arch-image"
               alt="Learning Framework"/>
<ul>
  <li>
    <strong>Finetuning visual encoders</strong> improves in-domain performance but may harm cross-embodiment transfer.
  </li>
  <li>
    <strong>The modular design</strong> including modularized data and tokenization facilitates positive transfer from <span class="human-data">human</span> to <span class="robot-data">LocoMan</span>.

  </li>
</ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Acknowledgement</h2>
        <div class="content has-text-justified">
          <p>We thank Jianzhe Gu for the initial camera mount design. We are grateful to Yifeng Zhu, Xuxin Cheng, and Xiaolong Wang for their valuable discussions and constructive feedback. We also thank Alan Wang for his support during our experiments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{niu2025human2locoman,
  title={Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining},
  author={Niu, Yaru and Zhang, Yunzhe and Yu, Mingyang and Lin, Changyi and Li, Chenhao and Wang, Yikai and Yang, Yuxiang and Yu, Wenhao and Zhang, Tingnan and Li, Zhenzhen and Tan, Jie and Zhao, Ding},
  booktitle={Robotics: Science and Systems (RSS)},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            <!-- This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website. -->
            Template borrowed from <a href="https://nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
